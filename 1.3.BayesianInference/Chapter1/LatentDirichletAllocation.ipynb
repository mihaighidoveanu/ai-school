{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some IPython magic\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pymc as pm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "\n",
    "- We take the documents in form of strings, remove the punctuation marks and split the strings in words\n",
    "- Label the words of the documents by indexing them in order of their appearance \n",
    "    + e.g. \"My friend is not my enemy\" becomes ( 0, 1, 2, 3, 0, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "d1 = \"I had a peanuts butter sandwich for breakfast.\"\n",
    "d2 = \"I like to eat almonds, peanuts and walnuts.\"\n",
    "d3 = \"My neighbor got a little dog yesterday.\"\n",
    "d4 = \"Cats and dogs are mortal enemies.\"\n",
    "d5 = \"You mustnâ€™t feed peanuts to your dog.\"\n",
    "docs = [d1, d2, d3, d4, d5]\n",
    "# create a translation table that will be used to map punctuation characters into empty strings\n",
    "remove_punctuation = str.maketrans('', '', string.punctuation) \n",
    "docs = [doc.translate(remove_punctuation).split() for doc in docs] # remove punctuation marks and split sentences in words\n",
    "\n",
    "def factorize_docs(docs):\n",
    "    index = 0\n",
    "    wdict = {}\n",
    "    data = []\n",
    "    for doc in docs:\n",
    "        doc_data = []\n",
    "        for word in doc:\n",
    "            if word not in wdict:\n",
    "                wdict[word] = index\n",
    "                index += 1\n",
    "            doc_data.append(wdict[word])\n",
    "        data.append(doc_data)\n",
    "    return np.array(data), wdict\n",
    "\n",
    "data, wdict = factorize_docs(docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The generative model\n",
    "- extract the number of words and number of documents from the data\n",
    "- create the random variables according to the [LDA Algorithm](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2 # number of topics\n",
    "V = max(wdict.values()) + 1 # number of words in vocabulary; add one because we start indexing from 0\n",
    "M = len(data) # number of documents\n",
    "\n",
    "N = [len(doc) for doc in data] # number of words in each document\n",
    "\n",
    "#words per topic a priori distribution - V-dimensional\n",
    "beta = np.ones(V)\n",
    "phi = pm.Container([pm.CompletedDirichlet(\"phi_%s\" % topic, pm.Dirichlet(\"pphi_%s\" % topic, theta=beta)) for topic in range(K)])\n",
    "\n",
    "# topics per document a priori distribution - K-dimensional \n",
    "alpha = np.ones(K)\n",
    "theta = pm.Container([pm.CompletedDirichlet(\"theta_%s\" % doc, pm.Dirichlet(\"ptheta_%s\" % doc, theta=alpha)) for doc in range(M)])\n",
    "\n",
    "# draw topics for each word of each document - a categorical for each word of the doc, based on the document's topic distribution\n",
    "z = pm.Container([pm.Categorical('z_%i' % doc, \n",
    "                     p = theta[doc], \n",
    "                     size=N[doc],\n",
    "                     value=np.random.randint(K, size=N[doc]))\n",
    "                  for doc in range(M)])\n",
    "\n",
    "# draw phisical words from word distributions according to each topic\n",
    "w = pm.Container([pm.Categorical(\"w_%i_%i\" % (doc,word),\n",
    "                    p = pm.Lambda('phi_z_%i_%i' % (doc,word), \n",
    "                              lambda z = z[doc][word], phi = phi : phi[z]),\n",
    "                    value = data[doc][word], \n",
    "                    observed = True)\n",
    "                  for doc in range(M) for word in range(N[doc])])\n",
    "\n",
    "model = pm.Model([theta, phi, z, w])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracing results\n",
    "\n",
    "- use mcmc to sample from the model \n",
    "- show the percentages with which each document corresponds to a certain topic\n",
    "- show the most relevant words of a certain topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc = pm.MCMC(model)\n",
    "mcmc.sample(40000, 10000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the traces\n",
    "phi_tr = [ mcmc.trace('phi_%i' % topic )[:][:, 0] for topic in range(K)]\n",
    "# plt.hist(phi[:,0])\n",
    "theta_tr = [ mcmc.trace('theta_%i' % doc)[:][:, 0] for doc in range(M)]\n",
    "z_tr = [ mcmc.trace('z_%i' % doc )[:] for doc in range(M)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Procentele cu care fiecare document apartine fiecarui topic : \")\n",
    "for doc in range(M):\n",
    "    print('Documentul {} : {}'.format(doc, np.mean(theta_tr[doc], axis = 0).tolist()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdict_inv = {v : k for k, v in wdict.items()} # reverse mapping from word - index to index - word\n",
    "no_words = 3 # number of words to show from each topic\n",
    "\n",
    "print(\"Cele mai importante {} cuvinte din fiecare topic :\".format(no_words))\n",
    "for topic in range(K):\n",
    "    most_important_index = np.argsort(phi_tr[topic].mean(axis = 0))[-no_words:]\n",
    "    most_important_words = [wdict_inv[index] for index in most_important_index ]\n",
    "    print(\"Topicul {} : {}\".format(topic, most_important_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
